{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## I3D网络介绍\n",
    "I3D（Inflated 3D ConvNet）是一种用于视频分类和行为识别的深度学习网络，是在二维卷积神经网络（CNN）的基础上扩展成为三维卷积神经网络。I3D网络最初由谷歌的研究团队在2017年提出，并在许多视频分类和行为识别任务中取得了领先的结果。\n",
    "与传统的二维卷积神经网络不同，I3D网络可以同时对时空信息进行建模，因此可以更好地处理视频数据。I3D网络的主要创新点在于其使用了充气策略来初始化网络权重，这意味着它可以将在图像分类任务中训练的二维CNN的权重用作其第一层的权重，并且可以直接从预先训练的二维CNN模型中导入权重。\n",
    "I3D网络的结构包括两个阶段。第一阶段是基于充气策略初始化的三维卷积神经网络，它以两个二维卷积神经网络为基础，分别对时间和空间进行卷积。第二阶段是一个全局平均池化层和一个分类器，用于对特征进行聚合和分类。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据集加载\n",
    "Kinetics400数据集包含400个行为类别的视频片段，是一个大规模的视频行为识别数据集。每个视频片段的长度在10秒到30秒之间，视频的分辨率为240x320或320x240。\n",
    "通过ParseKinetic400接口来加载Kinetics400数据集。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from src.data import transforms\n",
    "from src.data.meta import ParseDataset\n",
    "from src.data.video_dataset import VideoDataset\n",
    "\n",
    "from src.utils.class_factory import ClassFactory, ModuleType\n",
    "\n",
    "__all__ = [\"Kinetic400\", \"ParseKinetic400\"]\n",
    "\n",
    "\n",
    "@ClassFactory.register(ModuleType.DATASET)\n",
    "class Kinetic400(VideoDataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path (string): Root directory of the Mnist dataset or inference image.\n",
    "        split (str): The dataset split supports \"train\", \"test\" or \"infer\". Default: None.\n",
    "        transform (callable, optional): A function transform that takes in a video. Default:None.\n",
    "        target_transform (callable, optional): A function transform that takes in a label.\n",
    "                Default: None.\n",
    "        seq(int): The number of frames of captured video. Default: 16.\n",
    "        seq_mode(str): The way of capture video frames,\"part\") or \"discrete\" fetch. Default: \"part\".\n",
    "        align(boolean): The video contains multiple actions. Default: False.\n",
    "        batch_size (int): Batch size of dataset. Default:32.\n",
    "        repeat_num (int): The repeat num of dataset. Default:1.\n",
    "        shuffle (bool, optional): Whether or not to perform shuffle on the dataset. Default:None.\n",
    "        num_parallel_workers (int): Number of subprocess used to fetch the dataset in parallel.\n",
    "                Default: 1.\n",
    "        num_shards (int, optional): Number of shards that the dataset will be divided into.\n",
    "                Default: None.\n",
    "        shard_id (int, optional): The shard ID within num_shards. Default: None.\n",
    "        download (bool): Whether to download the dataset. Default: False.\n",
    "        frame_interval (int): Frame interval of the sample strategy. Default: 1.\n",
    "        num_clips (int): Number of clips sampled in one video. Default: 1.\n",
    "    Examples:\n",
    "        >>> from mindvision.msvideo.dataset.hmdb51 import HMDB51\n",
    "        >>> dataset = HMDB51(\"./data/\",\"train\")\n",
    "        >>> dataset = dataset.run()\n",
    "\n",
    "    The directory structure of Kinetic-400 dataset looks like:\n",
    "\n",
    "        .\n",
    "        |-kinetic-400\n",
    "            |-- train\n",
    "            |   |-- ___qijXy2f0_000011_000021.mp4      // video file\n",
    "            |   |-- ___dTOdxzXY_000022_000032.mp4      // video file\n",
    "            |    ...\n",
    "            |-- test\n",
    "            |   |-- __Zh0xijkrw_000042_000052.mp4       // video file\n",
    "            |   |-- __zVSUyXzd8_000070_000080.mp4   // video file\n",
    "            |-- val\n",
    "            |   |-- __wsytoYy3Q_000055_000065.mp4       // video file\n",
    "            |   |-- __vzEs2wzdQ_000026_000036.mp4   // video file\n",
    "            |    ...\n",
    "            |-- kinetics-400_train.csv             //training dataset label file.\n",
    "            |-- kinetics-400_test.csv              //testing dataset label file.\n",
    "            |-- kinetics-400_val.csv               //validation dataset label file.\n",
    "\n",
    "            ...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 path,\n",
    "                 split=None,\n",
    "                 transform=None,\n",
    "                 target_transform=None,\n",
    "                 seq=16,\n",
    "                 seq_mode=\"part\",\n",
    "                 align=False,\n",
    "                 batch_size=16,\n",
    "                 repeat_num=1,\n",
    "                 shuffle=None,\n",
    "                 num_parallel_workers=1,\n",
    "                 num_shards=None,\n",
    "                 shard_id=None,\n",
    "                 download=False,\n",
    "                 frame_interval=1,\n",
    "                 num_clips=1\n",
    "                 ):\n",
    "        load_data = ParseKinetic400(os.path.join(path, split)).parse_dataset\n",
    "        super().__init__(path=path,\n",
    "                         split=split,\n",
    "                         load_data=load_data,\n",
    "                         transform=transform,\n",
    "                         target_transform=target_transform,\n",
    "                         seq=seq,\n",
    "                         seq_mode=seq_mode,\n",
    "                         align=align,\n",
    "                         batch_size=batch_size,\n",
    "                         repeat_num=repeat_num,\n",
    "                         shuffle=shuffle,\n",
    "                         num_parallel_workers=num_parallel_workers,\n",
    "                         num_shards=num_shards,\n",
    "                         shard_id=shard_id,\n",
    "                         download=download,\n",
    "                         frame_interval=frame_interval,\n",
    "                         num_clips=num_clips\n",
    "                         )\n",
    "\n",
    "    @property\n",
    "    def index2label(self):\n",
    "        \"\"\"Get the mapping of indexes and labels.\"\"\"\n",
    "        csv_file = os.path.join(self.path, f\"kinetics-400_{self.split}.csv\")\n",
    "        mapping = []\n",
    "        with open(csv_file, \"r\")as f:\n",
    "            f_csv = csv.DictReader(f)\n",
    "            c = 0\n",
    "            for row in f_csv:\n",
    "                if not cls:\n",
    "                    cls = row['label']\n",
    "                    mapping.append(cls)\n",
    "                if row['label'] != cls:\n",
    "                    c += 1\n",
    "                    cls = row['label']\n",
    "                    mapping.append(cls)\n",
    "        return mapping\n",
    "\n",
    "    def download_dataset(self):\n",
    "        \"\"\"Download the HMDB51 data if it doesn't exist already.\"\"\"\n",
    "        raise ValueError(\"HMDB51 dataset download is not supported.\")\n",
    "\n",
    "    def default_transform(self):\n",
    "        \"\"\"Set the default transform for UCF101 dataset.\"\"\"\n",
    "        size = 256\n",
    "        order = (3, 0, 1, 2)\n",
    "        trans = [\n",
    "            transforms.VideoShortEdgeResize(size=size, interpolation='linear'),\n",
    "            transforms.VideoCenterCrop(size=(224, 224)),\n",
    "            transforms.VideoRescale(shift=0),\n",
    "            transforms.VideoReOrder(order=order),\n",
    "            transforms.VideoNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.255])\n",
    "        ]\n",
    "\n",
    "        return trans\n",
    "\n",
    "\n",
    "class ParseKinetic400(ParseDataset):\n",
    "    \"\"\"\n",
    "    Parse kinetic-400 dataset.\n",
    "    \"\"\"\n",
    "    urlpath = \"https://storage.googleapis.com/deepmind-media/Datasets/kinetics400.tar.gz\"\n",
    "\n",
    "    def load_cls_file(self):\n",
    "        \"\"\"Parse the category file.\"\"\"\n",
    "        base_path = os.path.dirname(self.path)\n",
    "        csv_file = os.path.join(base_path, f\"kinetics-400_train.csv\")\n",
    "        cls2id = {}\n",
    "        id2cls = []\n",
    "        cls_file = os.path.join(base_path, \"cls2index.json\")\n",
    "        print(cls_file)\n",
    "        if os.path.isfile(cls_file):\n",
    "            with open(cls_file, \"r\")as f:\n",
    "                cls2id = json.load(f)\n",
    "            id2cls = [*cls2id]\n",
    "            return id2cls, cls2id\n",
    "        with open(csv_file, \"r\")as f:\n",
    "            f_csv = csv.DictReader(f)\n",
    "            for row in f_csv:\n",
    "                if row['label'] not in cls2id:\n",
    "                    cls2id.setdefault(row['label'], len(cls2id))\n",
    "                    id2cls.append(row['label'])\n",
    "        f.close()\n",
    "        os.mknod(cls_file)\n",
    "        with open(cls_file, \"w\")as f:\n",
    "            f.write(json.dumps(cls2id))\n",
    "        return id2cls, cls2id\n",
    "\n",
    "    def parse_dataset(self, *args):\n",
    "        \"\"\"Traverse the HMDB51 dataset file to get the path and label.\"\"\"\n",
    "        parse_kinetic400 = ParseKinetic400(self.path)\n",
    "        split = os.path.split(parse_kinetic400.path)[-1]\n",
    "        video_label, video_path = [], []\n",
    "        _, cls2id = self.load_cls_file()\n",
    "        with open(os.path.join(os.path.dirname(parse_kinetic400.path),\n",
    "                               f\"kinetics-400_{split}.csv\"), \"rt\")as f:\n",
    "            f_csv = csv.DictReader(f)\n",
    "            for row in f_csv:\n",
    "                start = row['time_start'].zfill(6)\n",
    "                end = row['time_end'].zfill(6)\n",
    "                file_name = f\"{row['youtube_id']}_{start}_{end}.mp4\"\n",
    "                video_path.append(os.path.join(self.path, file_name))\n",
    "                video_label.append(cls2id[row['label']])\n",
    "        return video_path, video_label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 构建网络\n",
    "Inception3dModule是在I3D网络中使用的一种特殊的卷积神经网络模块，用于在三维卷积神经网络中处理视频数据。与传统的卷积神经网络不同，Inception3d Module使用了多个不同大小的卷积核，并将它们的输出连接起来，以提高模型的特征提取能力。以下是实现Inception3dModule的代码"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mindspore import nn\n",
    "from mindspore import ops\n",
    "from typing import Union, List, Tuple\n",
    "from src.models.avgpool3d import AvgPool3D\n",
    "\n",
    "from src.models.layers.unit3d import Unit3D\n",
    "from src.models.builder import build_layer,build_model\n",
    "from src.utils.class_factory import ClassFactory, ModuleType\n",
    "__all__ = ['I3D']\n",
    "\n",
    "@ClassFactory.register(ModuleType.LAYER)\n",
    "class Inception3dModule(nn.Cell):\n",
    "    \"\"\"\n",
    "    Inception3dModule definition.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int):  The number of channels of input frame images.\n",
    "        out_channels (int): The number of channels of output frame images.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        Inception3dModule(in_channels=3, out_channels=3)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Inception3dModule, self).__init__()\n",
    "        self.cat = ops.Concat(axis=1)\n",
    "        self.b0 = Unit3D(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels[0],\n",
    "            kernel_size=(1, 1, 1))\n",
    "        self.b1a = Unit3D(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels[1],\n",
    "            kernel_size=(1, 1, 1))\n",
    "        self.b1b = Unit3D(\n",
    "            in_channels=out_channels[1],\n",
    "            out_channels=out_channels[2],\n",
    "            kernel_size=(3, 3, 3))\n",
    "        self.b2a = Unit3D(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels[3],\n",
    "            kernel_size=(1, 1, 1))\n",
    "        self.b2b = Unit3D(\n",
    "            in_channels=out_channels[3],\n",
    "            out_channels=out_channels[4],\n",
    "            kernel_size=(3, 3, 3))\n",
    "        self.b3a = ops.MaxPool3D(\n",
    "            kernel_size=(3, 3, 3),\n",
    "            strides=(1, 1, 1),\n",
    "            pad_mode=\"same\")\n",
    "        self.b3b = Unit3D(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels[5],\n",
    "            kernel_size=(1, 1, 1))\n",
    "\n",
    "    def construct(self, x):\n",
    "        b0 = self.b0(x)\n",
    "        b1 = self.b1b(self.b1a(x))\n",
    "        b2 = self.b2b(self.b2a(x))\n",
    "        b3 = self.b3b(self.b3a(x))\n",
    "        return self.cat((b0, b1, b2, b3))\n",
    "\n",
    "\n",
    "@ClassFactory.register(ModuleType.LAYER)\n",
    "class InceptionI3d(nn.Cell):\n",
    "    \"\"\"\n",
    "    InceptionI3d architecture. TODO: i3d Inception backbone just in 3d?what about 2d. and two steam.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): The number of channels of input frame images(default 3).\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> InceptionI3d(in_channels=3)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=3):\n",
    "\n",
    "        super(InceptionI3d, self).__init__()\n",
    "\n",
    "        self.conv3d_1a_7x7 = Unit3D(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=64,\n",
    "            kernel_size=(7, 7, 7),\n",
    "            stride=(2, 2, 2))\n",
    "        self.maxpool3d_2a_3x3 = ops.MaxPool3D(\n",
    "            kernel_size=(1, 3, 3),\n",
    "            strides=(1, 2, 2),\n",
    "            pad_mode=\"same\")\n",
    "\n",
    "        self.conv3d_2b_1x1 = Unit3D(\n",
    "            in_channels=64,\n",
    "            out_channels=64,\n",
    "            kernel_size=(1, 1, 1))\n",
    "\n",
    "        self.conv3d_2c_3x3 = Unit3D(\n",
    "            in_channels=64,\n",
    "            out_channels=192,\n",
    "            kernel_size=(3, 3, 3))\n",
    "\n",
    "        self.maxpool3d_3a_3x3 = ops.MaxPool3D(\n",
    "            kernel_size=(1, 3, 3),\n",
    "            strides=(1, 2, 2),\n",
    "            pad_mode=\"same\")\n",
    "\n",
    "        self.mixed_3b = build_layer(\n",
    "            {\n",
    "                \"type\": \"Inception3dModule\",\n",
    "                \"in_channels\": 192,\n",
    "                \"out_channels\": [64, 96, 128, 16, 32, 32]})\n",
    "\n",
    "        self.mixed_3c = build_layer(\n",
    "            {\n",
    "                \"type\": \"Inception3dModule\",\n",
    "                \"in_channels\": 256,\n",
    "                \"out_channels\": [128, 128, 192, 32, 96, 64]})\n",
    "\n",
    "        self.maxpool3d_4a_3x3 = ops.MaxPool3D(\n",
    "            kernel_size=(3, 3, 3),\n",
    "            strides=(2, 2, 2),\n",
    "            pad_mode=\"same\")\n",
    "\n",
    "        self.mixed_4b = build_layer(\n",
    "            {\n",
    "                \"type\": \"Inception3dModule\",\n",
    "                \"in_channels\": 128 + 192 + 96 + 64,\n",
    "                \"out_channels\": [192, 96, 208, 16, 48, 64]})\n",
    "\n",
    "        self.mixed_4c = build_layer(\n",
    "            {\n",
    "                \"type\": \"Inception3dModule\",\n",
    "                \"in_channels\": 192 + 208 + 48 + 64,\n",
    "                \"out_channels\": [160, 112, 224, 24, 64, 64]})\n",
    "\n",
    "        self.mixed_4d = build_layer(\n",
    "            {\n",
    "                \"type\": \"Inception3dModule\",\n",
    "                \"in_channels\": 160 + 224 + 64 + 64,\n",
    "                \"out_channels\": [128, 128, 256, 24, 64, 64]})\n",
    "\n",
    "        self.mixed_4e = build_layer(\n",
    "            {\n",
    "                \"type\": \"Inception3dModule\",\n",
    "                \"in_channels\": 128 + 256 + 64 + 64,\n",
    "                \"out_channels\": [112, 144, 288, 32, 64, 64]})\n",
    "\n",
    "        self.mixed_4f = build_layer(\n",
    "            {\n",
    "                \"type\": \"Inception3dModule\",\n",
    "                \"in_channels\": 112 + 288 + 64 + 64,\n",
    "                \"out_channels\": [256, 160, 320, 32, 128, 128]})\n",
    "\n",
    "        self.maxpool3d_5a_2x2 = ops.MaxPool3D(\n",
    "            kernel_size=(2, 2, 2),\n",
    "            strides=(2, 2, 2),\n",
    "            pad_mode=\"same\")\n",
    "\n",
    "        self.mixed_5b = build_layer(\n",
    "            {\n",
    "                \"type\": \"Inception3dModule\",\n",
    "                \"in_channels\": 256 + 320 + 128 + 128,\n",
    "                \"out_channels\": [256, 160, 320, 32, 128, 128]})\n",
    "\n",
    "        self.mixed_5c = build_layer(\n",
    "            {\n",
    "                \"type\": \"Inception3dModule\",\n",
    "                \"in_channels\": 256 + 320 + 128 + 128,\n",
    "                \"out_channels\": [384, 192, 384, 48, 128, 128]})\n",
    "\n",
    "        self.mean_op = ops.ReduceMean(keep_dims=True)\n",
    "        self.concat_op = ops.Concat(axis=2)\n",
    "        self.stridedslice_op = ops.StridedSlice()\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Average pooling 3D construct.\"\"\"\n",
    "        x = self.conv3d_1a_7x7(x)\n",
    "        x = self.maxpool3d_2a_3x3(x)\n",
    "        x = self.conv3d_2b_1x1(x)\n",
    "        x = self.conv3d_2c_3x3(x)\n",
    "        x = self.maxpool3d_3a_3x3(x)\n",
    "        x = self.mixed_3b(x)\n",
    "        x = self.mixed_3c(x)\n",
    "        x = self.maxpool3d_4a_3x3(x)\n",
    "        x = self.mixed_4b(x)\n",
    "        x = self.mixed_4c(x)\n",
    "        x = self.mixed_4d(x)\n",
    "        x = self.mixed_4e(x)\n",
    "        x = self.mixed_4f(x)\n",
    "        x = self.maxpool3d_5a_2x2(x)\n",
    "        x = self.mixed_5b(x)\n",
    "        x = self.mixed_5c(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "平均3D池化（Average 3D Pooling）是一种在三维卷积神经网络中常用的操作，它通过将特征图中每个3D卷积核对应的区域取平均值来减小特征图的尺寸。与2D池化类似，平均3D池化可以降低模型的计算量，减少特征图的尺寸，并提高模型的鲁棒性和泛化能力。下面的代码实现平均3D池化："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AvgPooling3D(nn.Cell):\n",
    "    \"\"\"\n",
    "    A module of average pooling for 3D video features.\n",
    "\n",
    "    Args:\n",
    "        kernel_size(Union[int, List[int], Tuple[int]]): The size of kernel window used to take the\n",
    "            average value, Default: (1, 1, 1).\n",
    "        strides(Union[int, List[int], Tuple[int]]): The distance of kernel moving. Default: (1, 1, 1).\n",
    "\n",
    "    Inputs:\n",
    "        x(Tensor): The input Tensor.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, the pooled Tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 kernel_size: Union[int, List[int], Tuple[int]] = (1, 1, 1),\n",
    "                 strides: Union[int, List[int], Tuple[int]] = (1, 1, 1),\n",
    "                 ) -> None:\n",
    "        super(AvgPooling3D, self).__init__()\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size, kernel_size)\n",
    "        kernel_size = tuple(kernel_size)\n",
    "        if isinstance(strides, int):\n",
    "            strides = (strides, strides, strides)\n",
    "        strides = tuple(strides)\n",
    "\n",
    "        self.pool = AvgPool3D(kernel_size, strides)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.pool(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通过Inception3dModule与平均3D池化进行I3D头部网络与网络整体的构建"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@ClassFactory.register(ModuleType.LAYER)\n",
    "class I3dHead(nn.Cell):\n",
    "    \"\"\"\n",
    "    I3dHead definition\n",
    "\n",
    "    Args:\n",
    "        in_channels: Input channel.\n",
    "        num_classes (int): The number of classes .\n",
    "        dropout_keep_prob (float): A float value of prob.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        I3dHead(in_channels=2048, num_classes=400, dropout_keep_prob=0.5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes=400, dropout_keep_prob=0.5):\n",
    "        super(I3dHead, self).__init__()\n",
    "        self._num_classes = num_classes\n",
    "        self.dropout = nn.Dropout(dropout_keep_prob)\n",
    "        self.logits = Unit3D(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=self._num_classes,\n",
    "            kernel_size=(1, 1, 1),\n",
    "            activation=None,\n",
    "            norm=None,\n",
    "            has_bias=True)\n",
    "        self.mean_op = ops.ReduceMean()\n",
    "        self.squeeze = ops.Squeeze(3)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.logits(self.dropout(x))\n",
    "        x = self.squeeze(self.squeeze(x))\n",
    "        x = self.mean_op(x, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "@ClassFactory.register(ModuleType.MODEL)\n",
    "class I3D(nn.Cell):\n",
    "    \"\"\"\n",
    "    TODO: introduction i3d network.\n",
    "\n",
    "    Args:\n",
    "        in_channel(int): Number of channel of input data. Default: 3.\n",
    "        num_classes(int): Number of classes, it is the size of classfication score for every sample,\n",
    "            i.e. :math:`CLASSES_{out}`. Default: 400.\n",
    "        keep_prob(float): Probability of dropout for multi-dense-layer head, the number of probabilities equals\n",
    "            the number of dense layers. Default: 0.5.\n",
    "        pooling_keep_dim: whether to keep dim when pooling. Default: True.\n",
    "        pretrained(bool): If `True`, it will create a pretrained model, the pretrained model will be loaded\n",
    "            from network. If `False`, it will create a i3d model with uniform initialization for weight and bias. Default: False.\n",
    "\n",
    "    Inputs:\n",
    "        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`.\n",
    "\n",
    "    Outputs:\n",
    "        Tensor of shape :math:`(N, CLASSES_{out})`.\n",
    "\n",
    "    Supported Platforms:\n",
    "        ``GPU``\n",
    "\n",
    "    Examples:\n",
    "        >>> import numpy as np\n",
    "        >>> import mindspore as ms\n",
    "        >>> from mindvision.msvideo.models import i3d\n",
    "        >>>\n",
    "        >>> net = i3d()\n",
    "        >>> x = ms.Tensor(np.ones([1, 3, 32, 224, 224]), ms.float32)\n",
    "        >>> output = net(x)\n",
    "        >>> print(output.shape)\n",
    "        (1, 400)\n",
    "\n",
    "    About i3d:\n",
    "\n",
    "    TODO: i3d introduction.\n",
    "\n",
    "    Citation:\n",
    "\n",
    "    .. code-block::\n",
    "\n",
    "        TODO: i3d Citation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channel: int = 3,\n",
    "                 num_classes: int = 400,\n",
    "                 keep_prob: float = 0.5,\n",
    "                 #pooling_keep_dim: bool = True,\n",
    "                 backbone_output_channel=1024):\n",
    "        super(I3D, self).__init__()\n",
    "\n",
    "        self.backbone = InceptionI3d(in_channels=in_channel)\n",
    "        #self.neck = ops.AvgPool3D(kernel_size=(2,7,7),strides=(1,1,1))\n",
    "        self.neck = AvgPooling3D(kernel_size=(2,7,7))\n",
    "        #self.neck = ops.ReduceMean(keep_dims=pooling_keep_dim)\n",
    "        self.head = I3dHead(in_channels=backbone_output_channel,\n",
    "                            num_classes=num_classes,\n",
    "                            dropout_keep_prob=keep_prob)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.neck(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 模型训练与评估\n",
    "本节对于以上构建的I3D网络进行训练与评估"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mindspore import context, load_checkpoint, load_param_into_net\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor\n",
    "from mindspore.train import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.communication.management import init, get_rank, get_group_size\n",
    "\n",
    "from src.utils.check_param import Validator, Rel\n",
    "from src.utils.config import parse_args, Config\n",
    "from src.loss.builder import build_loss\n",
    "from src.schedule.builder import get_lr\n",
    "from src.optim.builder import build_optimizer\n",
    "from src.data.builder import build_dataset, build_transforms\n",
    "from src.models import build_model\n",
    "\n",
    "def main(pargs):\n",
    "    # set config context\n",
    "    config = Config(pargs.config)\n",
    "    context.set_context(**config.context)\n",
    "\n",
    "    # run distribute\n",
    "    if config.train.run_distribute:\n",
    "        if config.device_target == \"Ascend\":\n",
    "            init()\n",
    "        else:\n",
    "            init(\"nccl\")\n",
    "        context.set_auto_parallel_context(device_num=get_group_size(),\n",
    "                                          parallel_mode=ParallelMode.DATA_PARALLEL,\n",
    "                                          gradients_mean=True)\n",
    "        ckpt_save_dir = config.train.ckpt_path + \"ckpt_\" + str(get_rank()) + \"/\"\n",
    "    else:\n",
    "        ckpt_save_dir = config.train.ckpt_path\n",
    "\n",
    "    # perpare dataset\n",
    "    transforms = build_transforms(config.data_loader.train.map.operations)\n",
    "    data_set = build_dataset(config.data_loader.train.dataset)\n",
    "    data_set.transform = transforms\n",
    "    dataset_train = data_set.run()\n",
    "    Validator.check_int(dataset_train.get_dataset_size(), 0, Rel.GT)\n",
    "    batches_per_epoch = dataset_train.get_dataset_size()\n",
    "\n",
    "    # set network\n",
    "    network = build_model(config.model)\n",
    "\n",
    "    # set loss\n",
    "    network_loss = build_loss(config.loss)\n",
    "    # set lr\n",
    "    lr_cfg = config.learning_rate\n",
    "    lr_cfg.steps_per_epoch = int(batches_per_epoch / config.data_loader.group_size)\n",
    "    lr = get_lr(lr_cfg)\n",
    "\n",
    "    # set optimizer\n",
    "    config.optimizer.params = network.trainable_params()\n",
    "    config.optimizer.learning_rate = lr\n",
    "    network_opt = build_optimizer(config.optimizer)\n",
    "\n",
    "    if config.train.pre_trained:\n",
    "        # load pretrain model\n",
    "        param_dict = load_checkpoint(config.train.pretrained_model)\n",
    "        load_param_into_net(network, param_dict)\n",
    "\n",
    "    # set checkpoint for the network\n",
    "    ckpt_config = CheckpointConfig(\n",
    "        save_checkpoint_steps=config.train.save_checkpoint_steps,\n",
    "        keep_checkpoint_max=config.train.keep_checkpoint_max)\n",
    "    ckpt_callback = ModelCheckpoint(prefix=config.model_name,\n",
    "                                    directory=ckpt_save_dir,\n",
    "                                    config=ckpt_config)\n",
    "\n",
    "    # init the whole Model\n",
    "    model = Model(network,\n",
    "                  network_loss,\n",
    "                  network_opt,\n",
    "                  metrics={\"Accuracy\": Accuracy()})\n",
    "\n",
    "    # begin to train\n",
    "    print('[Start training `{}`]'.format(config.model_name))\n",
    "    print(\"=\" * 80)\n",
    "    model.train(config.train.epochs,\n",
    "                dataset_train,\n",
    "                callbacks=[ckpt_callback, LossMonitor()],\n",
    "                dataset_sink_mode=config.dataset_sink_mode)\n",
    "    print('[End of training `{}`]'.format(config.model_name))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    main(args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 可视化模型预测\n",
    "本节对于训练后模型进行可视化预测并输出结果"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import decord\n",
    "import moviepy.editor as mpy\n",
    "\n",
    "import mindspore as ms\n",
    "from mindspore import context, nn, load_checkpoint, load_param_into_net, ops, Tensor\n",
    "from mindspore.train import Model\n",
    "from mindspore.dataset.vision import py_transforms as T_p\n",
    "\n",
    "from src.utils.check_param import Validator, Rel\n",
    "from src.utils.config import parse_args, Config\n",
    "from src.loss.builder import build_loss\n",
    "from src.data.builder import build_dataset, build_transforms\n",
    "from src.models import build_model\n",
    "\n",
    "\n",
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 0.6\n",
    "FONTCOLOR = (255, 255, 255)\n",
    "BGBLUE = (0, 119, 182)\n",
    "THICKNESS = 1\n",
    "LINETYPE = 1\n",
    "\n",
    "\n",
    "def infer_classification(pargs):\n",
    "    # set config context\n",
    "    config = Config(pargs.config)\n",
    "    context.set_context(**config.context)\n",
    "    cast = ops.Cast()\n",
    "    # perpare dataset\n",
    "    transforms = build_transforms(config.data_loader.eval.map.operations)\n",
    "    #transforms = T_p.ToTensor()\n",
    "    data_set = build_dataset(config.data_loader.eval.dataset)\n",
    "    data_set.transform = transforms\n",
    "    dataset_infer = data_set.run()\n",
    "    Validator.check_int(dataset_infer.get_dataset_size(), 0, Rel.GT)\n",
    "\n",
    "    # set network\n",
    "    network = build_model(config.model)\n",
    "\n",
    "    # load pretrain model\n",
    "    param_dict = load_checkpoint(config.infer.pretrained_model)\n",
    "    load_param_into_net(network, param_dict)\n",
    "\n",
    "    # set loss\n",
    "    network_loss = build_loss(config.loss)\n",
    "\n",
    "    # init the whole Model\n",
    "    model = Model(network,\n",
    "                  network_loss)\n",
    "    expand_dims = ops.ExpandDims()\n",
    "    concat = ops.Concat(axis=0)\n",
    "\n",
    "    # 随机生成一个指定视频\n",
    "    vis_num = len(data_set.video_path)\n",
    "    vid_idx = np.random.randint(vis_num)\n",
    "    video_path = data_set.video_path[vid_idx]\n",
    "    video_reader = decord.VideoReader(video_path, num_threads=1)\n",
    "    img_set = []\n",
    "\n",
    "    for k in range(16):\n",
    "        im = video_reader[k].asnumpy()\n",
    "        img_set.append(im)\n",
    "    video = np.stack(img_set, axis=0)\n",
    "    # video = video.transpose(3, 0, 1, 2)\n",
    "    # video = Tensor(video, ms.float32)\n",
    "    # video = expand_dims(video, 0)\n",
    "    for t in transforms:\n",
    "        video = t(video)\n",
    "    # Begin to eval.\n",
    "    video = Tensor(video, ms.float32)\n",
    "    video = expand_dims(video, 0)\n",
    "    result = network(video)\n",
    "    result.asnumpy()\n",
    "    print(\"This is {}-th category\".format(result.argmax()))\n",
    "\n",
    "    return result, video_path\n",
    "\n",
    "def add_label(frame, label, BGCOLOR=BGBLUE):\n",
    "    threshold = 30\n",
    "    def split_label(label):\n",
    "        label = label.split()\n",
    "        lines, cline = [], ''\n",
    "        for word in label:\n",
    "            if len(cline) + len(word) < threshold:\n",
    "                cline = cline + ' ' + word\n",
    "            else:\n",
    "                lines.append(cline)\n",
    "                cline = word\n",
    "        if cline != '':\n",
    "            lines += [cline]\n",
    "        return lines\n",
    "\n",
    "    if len(label) > 30:\n",
    "        label = split_label(label)\n",
    "    else:\n",
    "        label = [label]\n",
    "    label = ['Action: '] + label\n",
    "\n",
    "    sizes = []\n",
    "    for line in label:\n",
    "        sizes.append(cv2.getTextSize(line, FONTFACE, FONTSCALE, THICKNESS)[0])\n",
    "    box_width = max([x[0] for x in sizes]) + 10\n",
    "    text_height = sizes[0][1]\n",
    "    box_height = len(sizes) * (text_height + 6)\n",
    "\n",
    "    cv2.rectangle(frame, (0, 0), (box_width, box_height), BGCOLOR, -1)\n",
    "    for i, line in enumerate(label):\n",
    "        location = (5, (text_height + 6) * i + text_height + 3)\n",
    "        cv2.putText(frame, line, location, FONTFACE, FONTSCALE, FONTCOLOR, THICKNESS, LINETYPE)\n",
    "    return frame\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import json\n",
    "    cls_file = '/home/publicfile/kinetics-400/cls2index.json'\n",
    "    with open(cls_file, \"r\")as f:\n",
    "        cls2id = json.load(f)\n",
    "\n",
    "    className = {v:k for k, v in cls2id.items()}\n",
    "\n",
    "    args = parse_args()\n",
    "    result, video_path = infer_classification(args)\n",
    "\n",
    "    label = className[int(result.argmax())]\n",
    "\n",
    "    video = decord.VideoReader(video_path)\n",
    "    frames = [x.asnumpy() for x in video]\n",
    "    vid_frames = []\n",
    "    for i in range(1, 50):\n",
    "        vis_frame = add_label(frames[i], label)\n",
    "        vid_frames.append(vis_frame)\n",
    "\n",
    "    vid = mpy.ImageSequenceClip(vid_frames, fps=24)\n",
    "    vid.write_gif('/home/i3d_mindspore-main/src/result.gif')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}