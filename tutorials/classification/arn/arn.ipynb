{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARN\n",
    "## 算法原理简介\n",
    "论文地址：[\\[2001.03905\\] Few-shot Action Recognition with Permutation-invariant Attention (ECCV 2020)](https://link.springer.com/chapter/10.1007/978-3-030-58558-7_31)\n",
    "\n",
    "1. ARN是用于少样本动作分类的网络，实现少样本分类的关键就在于相似度学习模块的构建。这一模块采用relation network，自动学习度量函数，输入查询集（query set）样本的二维特征图和支持集（support set）中每一类样本的二维特征图沿着channel维度作级联后得到的特征对，即可输出每一个样本对的相似度得分。具体的，这一相似度度量模块主要由四层Conv3d构成，进行线性变换。\n",
    "\n",
    "2. ARN包含特征编码器、自监督学习模块、注意力机制、基于增强和对齐的注意力机制、特征图聚合、少样本学习六个主要部分，如下：\n",
    "\n",
    "- 特征编码器\n",
    "\n",
    "特征提取对任何一个深度神经网络模型都是至关重要的一步，特征提取的准确性和针对性直接影响了模型对输入数据的采样和感知能力，一定程度上决定了模型表现的“上限”。ARN采用基于C3D结构的特征编码器Conv-4-64，即4层卷积、64输出通道，是对C3D的简化和优化。输入视频数据 **V** （如20帧）经过编码器特征提取操作 **f** 后输出维度为 **C\\*T\\*H\\*W** 的特征向量 **Φ** 。\n",
    "\n",
    "具体而言，编码器由四层卷积块组成，前两层卷积块计算后分别插入一层kernel和stride均为2\\*2\\*2的最大池化计算使输入数据在 **T H W** 维度上均缩小为原来的1/4。其中，每个卷积块依此包含卷积核大小为3\\*3\\*3且输出通道为64的卷积计算、批标准化（Batch Normalization）处理和ReLU激活函数处理。\n",
    "\n",
    "- 自监督学习模块\n",
    "\n",
    "将自监督学习模块应用于视频少样本分类，主要是为了鼓励基本网络模型学习附加的辅助任务，从而促使模型学到通用、稳定的特征提取方法，以更好地理解输入图像或视频等数据中蕴含的语义等信息。ARN的工作同时在时域和空域上引入自监督学习任务，在基于数据增强的设计思路指引下创建辅助任务（pretext）的学习。\n",
    "\n",
    "具体的，在时域上，不同于以往工作对每一帧进行乱序操作，是对不重叠的、固定长度的帧块进行乱序操作，即随机打乱动作块在时域上的先后顺序。这样做的好处在于更不容易拆解、打破判别性强的动作块所在相关联的帧，更好地保留了连贯的动作行为信息。在空域上，可以对视频中每一帧图像进行拼图或旋转的增强操作，如拼图操作将每一帧图像分为不重叠的四个区块再随机打乱，旋转操作则是将每一帧图像随机旋转0°、90°、180°或270°。\n",
    "\n",
    "- 注意力机制\n",
    "\n",
    "ARN中通过注意力机制来来实现为不同贡献度的动作块分配不同关注度，即不同的注意力权重向量。具体的，考虑到计算资源的限制，时空注意力单元被分为时间和空间两个模块，分别对每一帧图像的不同部分和不同时间段的动作块的不同贡献度作出描述。\n",
    "\n",
    "将维度为 **C\\*T\\*H\\*W** 的特征向量 输入到时间注意力单元，得到 **1\\*T\\*1\\*1** 维度的注意力向量，输入到空间注意力单元则得到 **1\\*1\\*H\\*W** 维度的注意力向量。时间和空间注意力作用影响程度通过两个参数来进行调控。其中 **t** 、 **s** 分别表示时间、空间注意力单元， **Φ\\*** 表示注意力特征图。\n",
    "\n",
    "- 基于增强和对齐的注意力机制\n",
    "\n",
    "基于增强和对齐的注意力机制（Augmentation-guided attention by alignment）是ARN工作中的主要创新点，模型实现的序列无关效果也主要得益于这个机制。如下图中所示，这一机制主要通过基于数据增强的自监督任务来加强时间和空间的注意力。具体的，通过将1）原始数据经过特征编码得到的表征先经过注意力单元的处理、再进行增强操作（如拼图或旋转）得到的“增强了的注意力向量”，2）增强后的视频数据经过特征编码得到的表征经由注意力单元处理后进一步得到注意力向量（即“增强数据的注意力向量”），这两个向量进行对齐学习来实施。这样打乱和对齐的做法旨在通过数据增强建立动作块和特定注意力权重之间的映射，使注意力机制能够适应输入序列的变化，调节、拓宽模型感受的范围，从而达到序列无关的提升效果。\n",
    "\n",
    "- 特征图聚合\n",
    "\n",
    "ARN中的聚合阶段采用带有功率归一化（Power Normalization, PN）的二阶池化（Second-order Pooling）操作。二阶池化通过算子 **g** 对特征向量进行降维操作得到特征图 **ψ** 。\n",
    "\n",
    "具体的，经过特殊编码器提取特征和注意力单元增强特征后得到特征向量维度为 **C\\*T\\*H\\*W** ，在将这些向量输入到二阶池化函数之前首先对其进行变形，成为 **C\\*N** ，其中 **N = T\\*H\\*W** ，便于后续池化计算。经过二阶池化的处理（核心为向量乘上本身的转置向量，即一个整形操作）输出得到维度为 **C\\*C** 的特征向量，即二维特征图，在实现降维的同时也通过 **N** 即 **T\\*H\\*W** 维度上的信息，体现了对输入数据的序列无关性，且对于输入视频帧长也不具约束。\n",
    "\n",
    "特别的，此处二阶池化操作还带有Power Normalization。从直观上看，加上PN操作的好处在于可以降低频繁出现的视觉元素对结果的贡献，而增加出现频率较低的视觉元素对结果的贡献，这也更加符合人类观察学习的特点。PN关注图像中的视觉元素是否共同出现，而非统计共同出现了几次，这大大降低了后续比较器需要记忆的数据量，而这也正是学习分类任务的自然特性之一。所以，PN引入二阶池化很适合少样本任务的学习。\n",
    "\n",
    "- 少样本学习————关系网络\n",
    "\n",
    "ARN中的比较器采用关系网络（Relation Network, RN）的结构，这一部分也是该网络结构适用于少样本学习的直接体现。在ARN网络结构中，比较器RN的输入为关系描述（relation descriptors）向量，即一对对的特征对（Feature Pair）。这些特征对由二阶池化后得到的二维特征图经过在通道维度上的进行级联拼接得到，维度为 **2\\*C\\*C** 。具体的，将拼接操作记为 **θ** ，是指查询集中的特征图和支持集中每一类别的特征图分别做通道维度上的级联的过程。特别的，如果不是n-way 1shot任务，即支持集中每一个类别包含不止一个样本，则先对这些样本在通道维度上做平均（或最大）级联，得到与各个样本具有相同维度 **C\\*C** 的特征图代表这一类别的图像特征，再与查询集中样本做上述级联。\n",
    "\n",
    "这些拼接好的特征对输入到RN（记为 **r** ）中进行相似度的比较，输出相似度得分高的类别作为预测类别从而实现对视频的分类。\n",
    "\n",
    "RN采用均方误差（Mean Square Error, MSE）作为损失函数对网络进行训练。\n",
    "\n",
    "\n",
    "3. ARN网络模型的总体框架图如下：\n",
    "\n",
    "![ARN_architecture](./pics/ARN_model.png)\n",
    "\n",
    "4. ARN实现视频少样本动作分类的可视化效果如下：\n",
    "\n",
    "![1](./pics/result-1.gif)\n",
    "![2](./pics/result-2.gif)\n",
    "![3](./pics/result-3.gif)\n",
    "![4](./pics/result-4.gif)\n",
    "![5](./pics/result-5.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境准备\n",
    "```text\n",
    "git clone https://gitee.com/yanlq46462828/zjut_mindvideo.git\n",
    "cd zjut_mindvideo\n",
    "\n",
    "# Please first install mindspore according to instructions on the official website: https://www.mindspore.cn/install\n",
    "\n",
    "pip install -r requirements.txt\n",
    "pip install -e .\n",
    "```\n",
    "### 训练流程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import nn\n",
    "from mindspore import context, load_checkpoint, load_param_into_net, ParallelMode\n",
    "from mindspore.communication.management import init, get_rank, get_group_size\n",
    "from mindspore.nn import MSELoss\n",
    "from mindspore.train import Model\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor\n",
    "\n",
    "from mindvideo.utils.check_param import Validator,Rel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 数据集准备\n",
    "\n",
    "下载[UCF101](https://www.crcv.ucf.edu/data/UCF101.php)数据集及其split文件至如\"/data0/ucf101-videos\"的路径下并解压。\n",
    "\n",
    "- UCF101数据集来自于Youtube，包含13320个动作视频段和101个动作类别。在这项工作中，数据集中的70个类被用于训练，10个类用于验证，剩余21个类用于测试。\n",
    "\n",
    "```text\n",
    ".\n",
    "└─ucf101                                    // contains 101 file folder\n",
    "  ├── ApplyEyeMakeup                        // contains 145 videos\n",
    "  │   ├── v_ApplyEyeMakeup_g01_c01.avi      // video file\n",
    "  │   ├── v_ApplyEyeMakeup_g01_c02.avi      // video file\n",
    "  │    ...\n",
    "  ├── ApplyLipstick                         // contains 114 image files\n",
    "  │   ├── v_ApplyLipstick_g01_c01.avi       // video file\n",
    "  │   ├── v_ApplyLipstick_g01_c02.avi       // video file\n",
    "  │    ...\n",
    "  ├── ucfTrainTestlist                      // contains category files\n",
    "  │   ├── classInd.txt                      // Category file.\n",
    "  │   ├── testlist01.txt                    // split file\n",
    "  │   ├── trainlist01.txt                   // split file\n",
    "  ...\n",
    "```\n",
    "\n",
    "##### 数据集加载\n",
    "\n",
    "通过基于VideoDataset编写的Ucf101类来加载ucf101数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvideo.data.ucf101 import UCF101\n",
    "# Data Pipeline.\n",
    "dataset_train = UCF101(path=\"/data0/ucf101-videos\",\n",
    "                               batch_size=1,\n",
    "                               split='train',\n",
    "                               shuffle=False,\n",
    "                               seq=16,\n",
    "                               num_parallel_workers=2,\n",
    "                               suffix=\"task\",\n",
    "                               task_num=100,\n",
    "                               task_n=5,\n",
    "                               #    task_k=5,\n",
    "                               task_k=1,\n",
    "                               task_q=1)\n",
    "ckpt_save_dir = './arn_output'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 数据处理\n",
    "\n",
    "通过VideoReshape、VideoResize、VideoNormalize等一系列变换对数据进行预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvideo.data.transforms.video_to_tensor import VideoToTensor\n",
    "from mindvideo.data.transforms import VideoResize, VideoReshape, VideoNormalize\n",
    "\n",
    "transforms = [\n",
    "    VideoReshape((-1, 240, 320, 3)),\n",
    "    VideoResize((128, 128)),\n",
    "    VideoToTensor(),\n",
    "    VideoNormalize((0.3474, 0.3474, 0.3474), (0.2100, 0.2100, 0.2100)),\n",
    "    VideoReshape((3, -1, 16, 128, 128))\n",
    "    ]\n",
    "\n",
    "dataset_train.transform = transforms\n",
    "dataset_train = dataset_train.run()\n",
    "\n",
    "\n",
    "Validator.check_int(dataset_train.get_dataset_size(), 0, Rel.GT)\n",
    "step_size = dataset_train.get_dataset_size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 网络构建\n",
    "\n",
    "ARN包含embedding、backbone、neck、head几部分，大致可以归纳为如下几点：\n",
    "\n",
    "- embedding部分：主要是一个类似于C3D结构的特征提取器，通过四层Conv3d进行线性变换。这一部分支持对C3D主干网络的复用，也支持直接使用Unit3d类（包含Conv3d、norm、activation）进行构建，仅需在入参is_c3d中指定True或False即可。\n",
    "\n",
    "- backbone部分：主要使用空间注意力模块对上一步embedding提取得到的特征进行增强，后将特征维度变换为C * N便于后续处理。空间注意力模块主要由三层Conv3d构成，进行线性变换。\n",
    "\n",
    "- neck部分：主要进行带有功率归一化（power normalization）的二阶池化（second-order pooling）处理，有助于实现序列无关以及少样本条件下的性能可靠度。后将特征维度变换为C * C便于特征图的拼接和特征对的构建，以实现最后基于度量的分类。\n",
    "\n",
    "- head部分：主要相似度学习模块，采用relation network实现，自动学习度量函数。具体如上第一点中所述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvideo.models.arn import ARN\n",
    "# Create model\n",
    "network = ARN()\n",
    "# network = ARN(support_num_per_class = 1,\n",
    "#               query_num_per_class = 1,\n",
    "#               class_num = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvideo.schedule.lr_schedule import warmup_cosine_annealing_lr_v1\n",
    "# Set learning rate scheduler.\n",
    "learning_rate = warmup_cosine_annealing_lr_v1(lr=0.01,\n",
    "                                                steps_per_epoch=step_size,\n",
    "                                                warmup_epochs=4,\n",
    "                                                max_epoch=100,\n",
    "                                                t_max=100,\n",
    "                                                eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function.\n",
    "network_loss = MSELoss()\n",
    "\n",
    "# Define optimizer.\n",
    "network_opt = nn.Adam(network.trainable_params(),\n",
    "                      learning_rate=learning_rate\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set checkpoint config for the network\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=step_size,\n",
    "                               keep_checkpoint_max=10\n",
    "                               )\n",
    "ckpt_callback = ModelCheckpoint(prefix=\"arn\",\n",
    "                                directory=ckpt_save_dir, \n",
    "                                config=ckpt_config\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvideo.utils.task_acc import TaskAccuracy\n",
    "\n",
    "# Init the model.\n",
    "model = Model(network, loss_fn=network_loss,\n",
    "              optimizer=network_opt, \n",
    "              metrics={\"Accuracy\": TaskAccuracy()}\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvideo.utils.callbacks import SaveCallback\n",
    "\n",
    "# Begin to train.\n",
    "print(\"[Start training arn.]\")\n",
    "print(\"=\" * 80)\n",
    "model.train(1,\n",
    "            dataset_train,\n",
    "            callbacks=[ckpt_callback, LossMonitor(),\n",
    "                       SaveCallback(model)],\n",
    "            dataset_sink_mode=False)\n",
    "print(\"[End of training arn.]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练结果（以100 step为例）如下："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[WARNING] ME(1332441:140509751179072,MainProcess):2023-02-28-21:47:05.234.1 [mindspore/dataset/core/validator_helpers.py:804] 'Compose' from mindspore.dataset.transforms.py_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Compose' from mindspore.dataset.transforms instead.\n",
    "[WARNING] ME(1332441:140509751179072,MainProcess):2023-02-28-21:47:05.152.662 [mindspore/dataset/core/validator_helpers.py:804] 'Compose' from mindspore.dataset.transforms.py_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Compose' from mindspore.dataset.transforms instead.\n",
    "[WARNING] ME(1332441:140509751179072,MainProcess):2023-02-28-21:47:05.153.578 [mindspore/dataset/core/validator_helpers.py:804] 'Compose' from mindspore.dataset.transforms.py_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Compose' from mindspore.dataset.transforms instead.\n",
    "[Start training `arn`]\n",
    "================================================================================\n",
    "epoch: 1 step: 1, loss is 0.25524941086769104\n",
    "epoch: 1 step: 2, loss is 0.25443845987319946\n",
    "epoch: 1 step: 3, loss is 0.2525935173034668\n",
    "epoch: 1 step: 4, loss is 0.25137537717819214\n",
    "epoch: 1 step: 5, loss is 0.250659704208374\n",
    "epoch: 1 step: 6, loss is 0.25001853704452515\n",
    "epoch: 1 step: 7, loss is 0.24962767958641052\n",
    "epoch: 1 step: 8, loss is 0.2493773251771927\n",
    "epoch: 1 step: 9, loss is 0.24855978786945343\n",
    "epoch: 1 step: 10, loss is 0.24793760478496552\n",
    "epoch: 1 step: 11, loss is 0.24683822691440582\n",
    "epoch: 1 step: 12, loss is 0.24549396336078644\n",
    "epoch: 1 step: 13, loss is 0.2443828135728836\n",
    "epoch: 1 step: 14, loss is 0.24277088046073914\n",
    "epoch: 1 step: 15, loss is 0.24066577851772308\n",
    "epoch: 1 step: 16, loss is 0.23903010785579681\n",
    "epoch: 1 step: 17, loss is 0.23652563989162445\n",
    "epoch: 1 step: 18, loss is 0.23434361815452576\n",
    "epoch: 1 step: 19, loss is 0.23169800639152527\n",
    "epoch: 1 step: 20, loss is 0.22891250252723694\n",
    "epoch: 1 step: 21, loss is 0.22681906819343567\n",
    "epoch: 1 step: 22, loss is 0.2234703004360199\n",
    "epoch: 1 step: 23, loss is 0.22008958458900452\n",
    "epoch: 1 step: 24, loss is 0.21733327209949493\n",
    "epoch: 1 step: 25, loss is 0.21292409300804138\n",
    "epoch: 1 step: 26, loss is 0.20746207237243652\n",
    "epoch: 1 step: 27, loss is 0.20180831849575043\n",
    "epoch: 1 step: 28, loss is 0.1985597163438797\n",
    "epoch: 1 step: 29, loss is 0.19297300279140472\n",
    "epoch: 1 step: 30, loss is 0.1902618706226349\n",
    "epoch: 1 step: 31, loss is 0.1824992299079895\n",
    "epoch: 1 step: 32, loss is 0.17900963127613068\n",
    "epoch: 1 step: 33, loss is 0.17172077298164368\n",
    "epoch: 1 step: 34, loss is 0.17226023972034454\n",
    "epoch: 1 step: 35, loss is 0.16584697365760803\n",
    "epoch: 1 step: 36, loss is 0.1619439274072647\n",
    "epoch: 1 step: 37, loss is 0.16240167617797852\n",
    "epoch: 1 step: 38, loss is 0.15948693454265594\n",
    "epoch: 1 step: 39, loss is 0.1618405431509018\n",
    "epoch: 1 step: 40, loss is 0.15997858345508575\n",
    "epoch: 1 step: 41, loss is 0.16068755090236664\n",
    "epoch: 1 step: 42, loss is 0.1610174924135208\n",
    "epoch: 1 step: 43, loss is 0.16091376543045044\n",
    "epoch: 1 step: 44, loss is 0.15834209322929382\n",
    "epoch: 1 step: 45, loss is 0.15753015875816345\n",
    "epoch: 1 step: 46, loss is 0.16051506996154785\n",
    "epoch: 1 step: 47, loss is 0.15898260474205017\n",
    "epoch: 1 step: 48, loss is 0.16005201637744904\n",
    "epoch: 1 step: 49, loss is 0.15491977334022522\n",
    "epoch: 1 step: 50, loss is 0.15762482583522797\n",
    "epoch: 1 step: 51, loss is 0.1578051596879959\n",
    "epoch: 1 step: 52, loss is 0.16365858912467957\n",
    "epoch: 1 step: 53, loss is 0.158821240067482\n",
    "epoch: 1 step: 54, loss is 0.16738960146903992\n",
    "epoch: 1 step: 55, loss is 0.16367670893669128\n",
    "epoch: 1 step: 56, loss is 0.15251144766807556\n",
    "epoch: 1 step: 57, loss is 0.16210150718688965\n",
    "epoch: 1 step: 58, loss is 0.15495999157428741\n",
    "epoch: 1 step: 59, loss is 0.15872594714164734\n",
    "epoch: 1 step: 60, loss is 0.16470317542552948\n",
    "epoch: 1 step: 61, loss is 0.15621140599250793\n",
    "epoch: 1 step: 62, loss is 0.16062337160110474\n",
    "epoch: 1 step: 63, loss is 0.143747016787529\n",
    "epoch: 1 step: 64, loss is 0.16764435172080994\n",
    "epoch: 1 step: 65, loss is 0.1761326938867569\n",
    "epoch: 1 step: 66, loss is 0.1519642472267151\n",
    "epoch: 1 step: 67, loss is 0.15278653800487518\n",
    "epoch: 1 step: 68, loss is 0.15374016761779785\n",
    "epoch: 1 step: 69, loss is 0.15694880485534668\n",
    "epoch: 1 step: 70, loss is 0.164311945438385\n",
    "epoch: 1 step: 71, loss is 0.14492562413215637\n",
    "epoch: 1 step: 72, loss is 0.15428179502487183\n",
    "epoch: 1 step: 73, loss is 0.16885729134082794\n",
    "epoch: 1 step: 74, loss is 0.15910828113555908\n",
    "epoch: 1 step: 75, loss is 0.16557292640209198\n",
    "epoch: 1 step: 76, loss is 0.14613883197307587\n",
    "epoch: 1 step: 77, loss is 0.15841558575630188\n",
    "epoch: 1 step: 78, loss is 0.14438843727111816\n",
    "epoch: 1 step: 79, loss is 0.1619483381509781\n",
    "epoch: 1 step: 80, loss is 0.15307335555553436\n",
    "epoch: 1 step: 81, loss is 0.1652490198612213\n",
    "epoch: 1 step: 82, loss is 0.16576547920703888\n",
    "epoch: 1 step: 83, loss is 0.15550260245800018\n",
    "epoch: 1 step: 84, loss is 0.15189872682094574\n",
    "epoch: 1 step: 85, loss is 0.14772748947143555\n",
    "epoch: 1 step: 86, loss is 0.15366052091121674\n",
    "epoch: 1 step: 87, loss is 0.15767520666122437\n",
    "epoch: 1 step: 88, loss is 0.1561850905418396\n",
    "epoch: 1 step: 89, loss is 0.14553754031658173\n",
    "epoch: 1 step: 90, loss is 0.1552981436252594\n",
    "epoch: 1 step: 91, loss is 0.17699776589870453\n",
    "epoch: 1 step: 92, loss is 0.15017244219779968\n",
    "epoch: 1 step: 93, loss is 0.1440081000328064\n",
    "epoch: 1 step: 94, loss is 0.17357569932937622\n",
    "epoch: 1 step: 95, loss is 0.1716315597295761\n",
    "epoch: 1 step: 96, loss is 0.18425917625427246\n",
    "epoch: 1 step: 97, loss is 0.16761977970600128\n",
    "epoch: 1 step: 98, loss is 0.16819722950458527\n",
    "epoch: 1 step: 99, loss is 0.1540897935628891\n",
    "[WARNING] MD(1332441,7fca27be4700,python):2023-02-28-21:48:42.483.020 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 80.2226%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
    "epoch: 1 step: 100, loss is 0.15505123138427734\n",
    "[End of training `arn`]\n",
    "97.83494901657104"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import context, load_checkpoint, load_param_into_net\n",
    "from mindspore.nn import MSELoss\n",
    "from mindspore.train import Model\n",
    "\n",
    "from mindvideo.models.arn import ARN\n",
    "from mindvideo.data.ucf101 import UCF101\n",
    "from mindvideo.data.transforms.video_to_tensor import VideoToTensor\n",
    "from mindvideo.data.transforms import VideoResize, VideoReshape, VideoNormalize\n",
    "\n",
    "from mindvideo.utils.task_acc import TaskAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.PYNATIVE_MODE,\n",
    "                        # mode=context.GRAPH_MODE,\n",
    "                        device_target=\"GPU\", device_id=3)\n",
    "\n",
    "# Data Pipeline.\n",
    "dataset_eval = UCF101(path=\"/data0/ucf101-videos\",\n",
    "                          batch_size=1,\n",
    "                          split='test',\n",
    "                          shuffle=False,\n",
    "                          seq=16,\n",
    "                          num_parallel_workers=1,\n",
    "                          suffix=\"task\",\n",
    "                          task_num=100,\n",
    "                          task_n=5,\n",
    "                          # task_k=5,\n",
    "                          task_k=1,\n",
    "                          task_q=1\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [\n",
    "    VideoReshape((-1, 240, 320, 3)),\n",
    "    VideoResize((128, 128)),\n",
    "    VideoToTensor(),\n",
    "    VideoNormalize((0.3474, 0.3474, 0.3474), (0.2100, 0.2100, 0.2100)),\n",
    "    VideoReshape((3, -1, 16, 128, 128))\n",
    "]\n",
    "dataset_eval.transform = transforms\n",
    "dataset_eval = dataset_eval.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载模型权重文件[arn.ckpt](https://zjuteducn-my.sharepoint.com/:u:/g/personal/201906010313_zjut_edu_cn/ER55hujI22BOkyjL5UrBVt0BfKx8lmeW5DRctx46tfZRkA?e=hdIZIu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model.\n",
    "network = ARN()\n",
    "# network = ARN(support_num_per_class=5)\n",
    "\n",
    "param_dict = load_checkpoint('/home/teddy20/arn/arn.ckpt')\n",
    "load_param_into_net(network, param_dict)\n",
    "\n",
    "# Define loss function.\n",
    "network_loss = MSELoss()\n",
    "\n",
    "# Init the model.\n",
    "model = Model(network, loss_fn=network_loss,\n",
    "              metrics={\"Accuracy\": TaskAccuracy()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin to eval.\n",
    "print('[Start eval arn]')\n",
    "result = model.eval(dataset_eval, dataset_sink_mode=False)\n",
    "print(\"result:\", result[\"Accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估结果如下："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[WARNING] ME(1555743:139829033322304,MainProcess):2023-03-01-14:06:02.735.845 [mindspore/dataset/core/validator_helpers.py:804] 'Compose' from mindspore.dataset.transforms.py_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Compose' from mindspore.dataset.transforms instead.\n",
    "[WARNING] ME(1555743:139829033322304,MainProcess):2023-03-01-14:06:02.954.038 [mindspore/train/model.py:1077] For EvalLossMonitor callback, {'epoch_begin', 'step_begin', 'step_end', 'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
    "[WARNING] ME(1555743:139829033322304,MainProcess):2023-03-01-14:06:02.955.695 [mindspore/dataset/core/validator_helpers.py:804] 'Compose' from mindspore.dataset.transforms.py_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Compose' from mindspore.dataset.transforms instead.\n",
    "[WARNING] ME(1555743:139829033322304,MainProcess):2023-03-01-14:06:04.472.762 [mindspore/dataset/core/validator_helpers.py:804] 'Compose' from mindspore.dataset.transforms.py_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Compose' from mindspore.dataset.transforms instead.\n",
    "[WARNING] ME(1555743:139829033322304,MainProcess):2023-03-01-14:06:06.410.772 [mindspore/dataset/core/validator_helpers.py:804] 'Compose' from mindspore.dataset.transforms.py_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Compose' from mindspore.dataset.transforms instead.\n",
    "[WARNING] ME(1555743:139829033322304,MainProcess):2023-03-01-14:06:06.446.055 [mindspore/dataset/core/validator_helpers.py:804] 'Compose' from mindspore.dataset.transforms.py_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Compose' from mindspore.dataset.transforms instead.\n",
    "[WARNING] ME(1555743:139829033322304,MainProcess):2023-03-01-14:06:11.538.827 [mindspore/dataset/core/validator_helpers.py:804] 'Compose' from mindspore.dataset.transforms.py_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Compose' from mindspore.dataset.transforms instead.\n",
    "step:[    1/  100], metrics:[], loss:[0.089/0.089], time:4985.604 ms, \n",
    "step:[    2/  100], metrics:['Accuracy: 0.8000'], loss:[0.118/0.103], time:108.598 ms, \n",
    "step:[    3/  100], metrics:['Accuracy: 0.9000'], loss:[0.147/0.118], time:732.968 ms, \n",
    "step:[    4/  100], metrics:['Accuracy: 0.7333'], loss:[0.127/0.120], time:82.494 ms, \n",
    "step:[    5/  100], metrics:['Accuracy: 0.8000'], loss:[0.148/0.126], time:114.944 ms, \n",
    "step:[    6/  100], metrics:['Accuracy: 0.7600'], loss:[0.151/0.130], time:847.090 ms, \n",
    "step:[    7/  100], metrics:['Accuracy: 0.6667'], loss:[0.161/0.134], time:962.741 ms, \n",
    "step:[    8/  100], metrics:['Accuracy: 0.6857'], loss:[0.074/0.127], time:936.868 ms, \n",
    "step:[    9/  100], metrics:['Accuracy: 0.7250'], loss:[0.214/0.137], time:908.237 ms, \n",
    "step:[   10/  100], metrics:['Accuracy: 0.7111'], loss:[0.170/0.140], time:925.588 ms, \n",
    "step:[   11/  100], metrics:['Accuracy: 0.7000'], loss:[0.134/0.139], time:854.217 ms, \n",
    "step:[   12/  100], metrics:['Accuracy: 0.6909'], loss:[0.126/0.138], time:826.660 ms, \n",
    "step:[   13/  100], metrics:['Accuracy: 0.6667'], loss:[0.091/0.135], time:949.906 ms, \n",
    "step:[   14/  100], metrics:['Accuracy: 0.6769'], loss:[0.111/0.133], time:941.617 ms, \n",
    "step:[   15/  100], metrics:['Accuracy: 0.6714'], loss:[0.131/0.133], time:952.303 ms, \n",
    "step:[   16/  100], metrics:['Accuracy: 0.6533'], loss:[0.110/0.131], time:911.044 ms, \n",
    "step:[   17/  100], metrics:['Accuracy: 0.6500'], loss:[0.273/0.140], time:924.091 ms, \n",
    "step:[   18/  100], metrics:['Accuracy: 0.6353'], loss:[0.189/0.143], time:958.892 ms, \n",
    "step:[   19/  100], metrics:['Accuracy: 0.6111'], loss:[0.090/0.140], time:985.099 ms, \n",
    "step:[   20/  100], metrics:['Accuracy: 0.6211'], loss:[0.069/0.136], time:921.965 ms, \n",
    "step:[   21/  100], metrics:['Accuracy: 0.6300'], loss:[0.140/0.136], time:961.677 ms, \n",
    "step:[   22/  100], metrics:['Accuracy: 0.6381'], loss:[0.134/0.136], time:867.552 ms, \n",
    "step:[   23/  100], metrics:['Accuracy: 0.6364'], loss:[0.159/0.137], time:806.244 ms, \n",
    "step:[   24/  100], metrics:['Accuracy: 0.6261'], loss:[0.126/0.137], time:995.279 ms, \n",
    "step:[   25/  100], metrics:['Accuracy: 0.6250'], loss:[0.160/0.138], time:935.043 ms, \n",
    "step:[   26/  100], metrics:['Accuracy: 0.6240'], loss:[0.107/0.137], time:814.355 ms, \n",
    "step:[   27/  100], metrics:['Accuracy: 0.6231'], loss:[0.158/0.137], time:837.804 ms, \n",
    "step:[   28/  100], metrics:['Accuracy: 0.6148'], loss:[0.128/0.137], time:862.694 ms, \n",
    "step:[   29/  100], metrics:['Accuracy: 0.6071'], loss:[0.091/0.135], time:929.604 ms, \n",
    "step:[   30/  100], metrics:['Accuracy: 0.6138'], loss:[0.134/0.135], time:937.966 ms, \n",
    "step:[   31/  100], metrics:['Accuracy: 0.6200'], loss:[0.097/0.134], time:916.139 ms, \n",
    "step:[   32/  100], metrics:['Accuracy: 0.6194'], loss:[0.099/0.133], time:984.950 ms, \n",
    "step:[   33/  100], metrics:['Accuracy: 0.6188'], loss:[0.123/0.133], time:915.211 ms, \n",
    "step:[   34/  100], metrics:['Accuracy: 0.6182'], loss:[0.214/0.135], time:1002.052 ms, \n",
    "step:[   35/  100], metrics:['Accuracy: 0.6118'], loss:[0.135/0.135], time:826.378 ms, \n",
    "step:[   36/  100], metrics:['Accuracy: 0.6057'], loss:[0.089/0.134], time:921.945 ms, \n",
    "step:[   37/  100], metrics:['Accuracy: 0.6167'], loss:[0.103/0.133], time:956.841 ms, \n",
    "step:[   38/  100], metrics:['Accuracy: 0.6216'], loss:[0.119/0.133], time:978.908 ms, \n",
    "step:[   39/  100], metrics:['Accuracy: 0.6211'], loss:[0.158/0.133], time:941.005 ms, \n",
    "step:[   40/  100], metrics:['Accuracy: 0.6103'], loss:[0.093/0.132], time:927.226 ms, \n",
    "step:[   41/  100], metrics:['Accuracy: 0.6150'], loss:[0.123/0.132], time:871.031 ms, \n",
    "step:[   42/  100], metrics:['Accuracy: 0.6195'], loss:[0.159/0.133], time:919.226 ms, \n",
    "step:[   43/  100], metrics:['Accuracy: 0.6143'], loss:[0.095/0.132], time:794.933 ms, \n",
    "step:[   44/  100], metrics:['Accuracy: 0.6140'], loss:[0.157/0.132], time:874.276 ms, \n",
    "step:[   45/  100], metrics:['Accuracy: 0.6136'], loss:[0.139/0.133], time:849.043 ms, \n",
    "step:[   46/  100], metrics:['Accuracy: 0.6089'], loss:[0.085/0.132], time:877.902 ms, \n",
    "step:[   47/  100], metrics:['Accuracy: 0.6130'], loss:[0.110/0.131], time:904.875 ms, \n",
    "step:[   48/  100], metrics:['Accuracy: 0.6128'], loss:[0.206/0.133], time:966.852 ms, \n",
    "step:[   49/  100], metrics:['Accuracy: 0.6042'], loss:[0.203/0.134], time:915.310 ms, \n",
    "step:[   50/  100], metrics:['Accuracy: 0.5959'], loss:[0.184/0.135], time:945.232 ms, \n",
    "step:[   51/  100], metrics:['Accuracy: 0.5880'], loss:[0.153/0.135], time:1005.336 ms, \n",
    "step:[   52/  100], metrics:['Accuracy: 0.5882'], loss:[0.141/0.136], time:846.246 ms, \n",
    "step:[   53/  100], metrics:['Accuracy: 0.5885'], loss:[0.123/0.135], time:959.409 ms, \n",
    "step:[   54/  100], metrics:['Accuracy: 0.5849'], loss:[0.170/0.136], time:920.870 ms, \n",
    "step:[   55/  100], metrics:['Accuracy: 0.5778'], loss:[0.163/0.136], time:785.633 ms, \n",
    "step:[   56/  100], metrics:['Accuracy: 0.5782'], loss:[0.175/0.137], time:802.633 ms, \n",
    "step:[   57/  100], metrics:['Accuracy: 0.5714'], loss:[0.052/0.136], time:970.165 ms, \n",
    "step:[   58/  100], metrics:['Accuracy: 0.5789'], loss:[0.085/0.135], time:829.369 ms, \n",
    "step:[   59/  100], metrics:['Accuracy: 0.5828'], loss:[0.049/0.133], time:810.693 ms, \n",
    "step:[   60/  100], metrics:['Accuracy: 0.5864'], loss:[0.183/0.134], time:985.595 ms, \n",
    "step:[   61/  100], metrics:['Accuracy: 0.5800'], loss:[0.117/0.134], time:906.079 ms, \n",
    "step:[   62/  100], metrics:['Accuracy: 0.5803'], loss:[0.114/0.133], time:952.803 ms, \n",
    "step:[   63/  100], metrics:['Accuracy: 0.5806'], loss:[0.106/0.133], time:929.201 ms, \n",
    "step:[   64/  100], metrics:['Accuracy: 0.5778'], loss:[0.116/0.133], time:805.948 ms, \n",
    "step:[   65/  100], metrics:['Accuracy: 0.5781'], loss:[0.119/0.133], time:787.724 ms, \n",
    "step:[   66/  100], metrics:['Accuracy: 0.5785'], loss:[0.086/0.132], time:789.349 ms, \n",
    "step:[   67/  100], metrics:['Accuracy: 0.5818'], loss:[0.131/0.132], time:950.360 ms, \n",
    "step:[   68/  100], metrics:['Accuracy: 0.5791'], loss:[0.175/0.133], time:951.824 ms, \n",
    "step:[   69/  100], metrics:['Accuracy: 0.5735'], loss:[0.144/0.133], time:917.203 ms, \n",
    "step:[   70/  100], metrics:['Accuracy: 0.5710'], loss:[0.173/0.133], time:909.480 ms, \n",
    "step:[   71/  100], metrics:['Accuracy: 0.5686'], loss:[0.202/0.134], time:808.212 ms, \n",
    "step:[   72/  100], metrics:['Accuracy: 0.5662'], loss:[0.154/0.134], time:906.659 ms, \n",
    "step:[   73/  100], metrics:['Accuracy: 0.5667'], loss:[0.145/0.135], time:919.868 ms, \n",
    "step:[   74/  100], metrics:['Accuracy: 0.5644'], loss:[0.128/0.135], time:862.657 ms, \n",
    "step:[   75/  100], metrics:['Accuracy: 0.5649'], loss:[0.142/0.135], time:831.783 ms, \n",
    "step:[   76/  100], metrics:['Accuracy: 0.5627'], loss:[0.142/0.135], time:883.641 ms, \n",
    "step:[   77/  100], metrics:['Accuracy: 0.5632'], loss:[0.109/0.134], time:820.826 ms, \n",
    "step:[   78/  100], metrics:['Accuracy: 0.5662'], loss:[0.122/0.134], time:934.502 ms, \n",
    "step:[   79/  100], metrics:['Accuracy: 0.5692'], loss:[0.086/0.134], time:780.328 ms, \n",
    "step:[   80/  100], metrics:['Accuracy: 0.5696'], loss:[0.068/0.133], time:892.666 ms, \n",
    "step:[   81/  100], metrics:['Accuracy: 0.5750'], loss:[0.063/0.132], time:941.636 ms, \n",
    "step:[   82/  100], metrics:['Accuracy: 0.5778'], loss:[0.127/0.132], time:910.576 ms, \n",
    "step:[   83/  100], metrics:['Accuracy: 0.5805'], loss:[0.142/0.132], time:877.943 ms, \n",
    "step:[   84/  100], metrics:['Accuracy: 0.5807'], loss:[0.182/0.133], time:909.617 ms, \n",
    "step:[   85/  100], metrics:['Accuracy: 0.5786'], loss:[0.090/0.132], time:899.963 ms, \n",
    "step:[   86/  100], metrics:['Accuracy: 0.5788'], loss:[0.170/0.133], time:886.544 ms, \n",
    "step:[   87/  100], metrics:['Accuracy: 0.5791'], loss:[0.126/0.132], time:912.352 ms, \n",
    "step:[   88/  100], metrics:['Accuracy: 0.5793'], loss:[0.083/0.132], time:802.856 ms, \n",
    "step:[   89/  100], metrics:['Accuracy: 0.5818'], loss:[0.158/0.132], time:751.491 ms, \n",
    "step:[   90/  100], metrics:['Accuracy: 0.5798'], loss:[0.106/0.132], time:766.607 ms, \n",
    "step:[   91/  100], metrics:['Accuracy: 0.5822'], loss:[0.159/0.132], time:780.416 ms, \n",
    "step:[   92/  100], metrics:['Accuracy: 0.5802'], loss:[0.137/0.132], time:853.709 ms, \n",
    "step:[   93/  100], metrics:['Accuracy: 0.5761'], loss:[0.119/0.132], time:904.019 ms, \n",
    "step:[   94/  100], metrics:['Accuracy: 0.5742'], loss:[0.101/0.132], time:944.246 ms, \n",
    "step:[   95/  100], metrics:['Accuracy: 0.5723'], loss:[0.095/0.131], time:931.043 ms, \n",
    "step:[   96/  100], metrics:['Accuracy: 0.5726'], loss:[0.130/0.131], time:863.324 ms, \n",
    "step:[   97/  100], metrics:['Accuracy: 0.5750'], loss:[0.182/0.132], time:926.642 ms, \n",
    "step:[   98/  100], metrics:['Accuracy: 0.5711'], loss:[0.161/0.132], time:868.583 ms, \n",
    "step:[   99/  100], metrics:['Accuracy: 0.5714'], loss:[0.140/0.132], time:937.576 ms, \n",
    "step:[  100/  100], metrics:['Accuracy: 0.5737'], loss:[0.138/0.132], time:865.490 ms, \n",
    "Epoch time: 91278.014 ms, per step time: 912.780 ms, avg loss: 0.132\n",
    "{'Accuracy': 0.574}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "代码仓库地址如下：\n",
    "\n",
    "Gitee   https://gitee.com/yanlq46462828/zjut_mindvideo\n",
    "\n",
    "Github  https://github.com/ZJUT-ERCISS/arn_mindspore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
